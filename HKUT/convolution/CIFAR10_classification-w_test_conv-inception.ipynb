{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision import datasets,transforms,utils\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])   #needed to do grayscale cob=nversion before normalizing to tensor\n",
    "trainset=datasets.CIFAR10('CIFAR10/',train=True,transform=transform,download=True)\n",
    "train_loader=DataLoader(trainset,batch_size=64,shuffle=True) #use all three channels\n",
    "\n",
    "testset=datasets.CIFAR10('CIFAR10/',train=False,transform=transform,download=True)\n",
    "test_loader=DataLoader(testset,batch_size=64,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ebce47d0b8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image,label=next(iter(train_loader))\n",
    "print(image.size())\n",
    "print(label.size())\n",
    "plt.imshow(image[0][0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  2,  9,  6,  1,  6,  4,  2,  6,  4,  4,  5,  3,  6,\n",
       "         9,  7,  4,  2,  1,  4,  8,  8,  2,  5,  1,  9,  8,  8,\n",
       "         0,  5,  1,  3,  7,  6,  8,  4,  9,  7,  2,  8,  6,  8,\n",
       "         8,  4,  7,  3,  1,  5,  3,  5,  0,  6,  0,  8,  6,  5,\n",
       "         2,  5,  9,  2,  3,  2,  1,  1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class inception_(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,input_):\n",
    "        super(inception_,self).__init__()\n",
    "           \n",
    "        self.conv1_1_1=torch.nn.Conv2d(input_,16,kernel_size=1)\n",
    "        \n",
    "        self.conv1_1_2=torch.nn.Conv2d(input_,16,kernel_size=1)\n",
    "        self.conv5_5_2=torch.nn.Conv2d(16,24,kernel_size=5,padding=2)\n",
    "        \n",
    "        self.conv1_1_3=torch.nn.Conv2d(input_,16,kernel_size=1) \n",
    "        self.conv3_3_1_3=torch.nn.Conv2d(16,24,kernel_size=3,padding=1)\n",
    "        self.conv3_3_2_3=torch.nn.Conv2d(24,24,kernel_size=3,padding=1)\n",
    "        \n",
    "        #self.avgpool4=torch.nn.AvgPool2d(kernel_size=3,stride=1,padding=1)\n",
    "        self.conv1_1_4=torch.nn.Conv2d(input_,24,kernel_size=1)\n",
    "\n",
    "   \n",
    "    def forward(self,x): \n",
    "        \n",
    "        #input_=x.size(0)\n",
    "        #x=x.view(-1)  ?\n",
    "        x1_1=self.conv1_1_1(x)\n",
    "        \n",
    "        x5_5=self.conv1_1_2(x)\n",
    "        x5_5=self.conv5_5_2(x5_5)\n",
    "        \n",
    "        x3_3=self.conv1_1_3(x)\n",
    "        x3_3=self.conv3_3_1_3(x3_3)\n",
    "        x3_3=self.conv3_3_2_3(x3_3)\n",
    "        \n",
    "        xpool=F.avg_pool2d(x,kernel_size=3,stride=1,padding=1)\n",
    "        xpool=self.conv1_1_4(xpool)\n",
    "        \n",
    "        outputs=[x1_1,x5_5,x3_3,xpool]\n",
    "        \n",
    "        return torch.cat(outputs,1)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Network,self).__init__()\n",
    "        self.conv1=torch.nn.Conv2d(3,10,kernel_size=5)\n",
    "        self.conv2=torch.nn.Conv2d(88,20,kernel_size=5)\n",
    "        \n",
    "        self.incept1=inception_(input_=10)\n",
    "        self.incept2=inception_(input_=20)\n",
    "        \n",
    "        self.mp=torch.nn.MaxPool2d(2)\n",
    "        self.fc=torch.nn.Linear(2200,10)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        input_size=x.size(0)\n",
    "        x=F.relu(self.mp(self.conv1(x)))\n",
    "        \n",
    "        x=self.incept1(x)\n",
    "        \n",
    "        x=F.relu(self.mp(self.conv2(x)))\n",
    "        \n",
    "        x=self.incept2(x)\n",
    "        \n",
    "        x=x.view(input_size,-1)\n",
    "        \n",
    "        x=self.fc(x)\n",
    "        y_pred=x\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "model=Network()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "criterion=torch.nn.CrossEntropyLoss()  #if increase the lr to 0.1, it kicks to local min and stays\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=0.01,momentum=0.9)  #had to change from SGD to Adam but later noy much difference\n",
    "lr_scheduler_=torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "#lr_scheduler_=torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10]) torch.Size([64])\n",
      "epoch 1 / 10 running_loss: 0.029214067105203866 Accuracy 30.890625\n",
      "epoch 1 / 10 running_loss: 0.026458006780594588 Accuracy 37.8828125\n",
      "epoch 1 / 10 running_loss: 0.024848014408101637 Accuracy 42.0625\n",
      "epoch 2 / 10 running_loss: 0.02374540111950162 Accuracy 44.97380356584298\n",
      "epoch 2 / 10 running_loss: 0.02283201101596039 Accuracy 47.30891918939204\n",
      "epoch 2 / 10 running_loss: 0.022138224578019304 Accuracy 49.009797790285596\n",
      "epoch 2 / 10 running_loss: 0.0215778627953871 Accuracy 50.41540110773629\n",
      "epoch 3 / 10 running_loss: 0.021109987050294876 Accuracy 51.60893024710666\n",
      "epoch 3 / 10 running_loss: 0.02061932660744924 Accuracy 52.8634973589102\n",
      "epoch 3 / 10 running_loss: 0.020274321052106227 Accuracy 53.76219664748562\n",
      "epoch 3 / 10 running_loss: 0.019954776537534944 Accuracy 54.55637366386173\n",
      "epoch 4 / 10 running_loss: 0.019636832508568767 Accuracy 55.38786362214576\n",
      "epoch 4 / 10 running_loss: 0.019344682601874653 Accuracy 56.087599846020595\n",
      "epoch 4 / 10 running_loss: 0.019119851685978538 Accuracy 56.638146725046916\n",
      "epoch 4 / 10 running_loss: 0.018907816035354737 Accuracy 57.188724876991074\n",
      "epoch 5 / 10 running_loss: 0.018703357062530044 Accuracy 57.70644354081952\n",
      "epoch 5 / 10 running_loss: 0.018494056264957873 Accuracy 58.223248454518696\n",
      "epoch 5 / 10 running_loss: 0.018333356989061366 Accuracy 58.62567764804003\n",
      "epoch 5 / 10 running_loss: 0.018186300080039013 Accuracy 58.995588622596784\n",
      "epoch 6 / 10 running_loss: 0.01802039099568813 Accuracy 59.41781357522677\n",
      "epoch 6 / 10 running_loss: 0.01785981639374091 Accuracy 59.799672326481975\n",
      "epoch 6 / 10 running_loss: 0.017717183965692846 Accuracy 60.159937446687515\n",
      "epoch 6 / 10 running_loss: 0.01760711247151576 Accuracy 60.42935817242317\n",
      "epoch 7 / 10 running_loss: 0.017485778693631392 Accuracy 60.72620164737775\n",
      "epoch 7 / 10 running_loss: 0.017373228098978278 Accuracy 60.97769242318086\n",
      "epoch 7 / 10 running_loss: 0.017261044768853713 Accuracy 61.247413627177366\n",
      "epoch 7 / 10 running_loss: 0.017171755211018547 Accuracy 61.470785840051896\n",
      "epoch 8 / 10 running_loss: 0.017061134377330738 Accuracy 61.748179096474374\n",
      "epoch 8 / 10 running_loss: 0.016957840404948448 Accuracy 61.98552569135856\n",
      "epoch 8 / 10 running_loss: 0.016872244476485932 Accuracy 62.18227407314734\n",
      "epoch 8 / 10 running_loss: 0.016784077793028405 Accuracy 62.409701763590135\n",
      "epoch 9 / 10 running_loss: 0.016699061793466568 Accuracy 62.62756099468251\n",
      "epoch 9 / 10 running_loss: 0.016618962282081005 Accuracy 62.832688808007276\n",
      "epoch 9 / 10 running_loss: 0.016534183800074088 Accuracy 63.03148918163085\n",
      "epoch 9 / 10 running_loss: 0.016462651685730884 Accuracy 63.19858986273949\n",
      "epoch 10 / 10 running_loss: 0.016380035196763173 Accuracy 63.39298126716019\n",
      "epoch 10 / 10 running_loss: 0.01630287913038268 Accuracy 63.56959895850945\n",
      "epoch 10 / 10 running_loss: 0.016244111057443632 Accuracy 63.70234254107266\n",
      "epoch 10 / 10 running_loss: 0.016187528578865493 Accuracy 63.84371090366663\n"
     ]
    }
   ],
   "source": [
    "#torch.set_printoptions(precision=2)\n",
    "def train_function(train_loader):\n",
    "    loss_running=0\n",
    "    count=0\n",
    "    count_batch=0\n",
    "    sum_acc=0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for image,label in iter(train_loader):\n",
    "\n",
    "            #input.resize_(input.size()[0], 784)   take to forward\n",
    "            y_pred=model(image)   #this is 64 (bacth_size)*10\n",
    "\n",
    "            if(count==0): print(y_pred.size(),label.size())\n",
    "            loss=criterion(y_pred,label)    #criterion(y_pred,label), crossentropy criterion need long (output of forward) and normal tensor (target)\n",
    "            loss_running=loss_running+loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            count=count+1\n",
    "            #y_pred_round=torch.round(y_pred)\n",
    "            count_batch=count_batch+(label.size()[0])\n",
    "            _,y_pred_=torch.max(y_pred,dim=1)    #argmax is the second value returned by torch.max()  ,this collapse dimension to batch size with argmax of probabililty/value (second) item, first one is the value itself \n",
    "\n",
    "            acc=(label==y_pred_).sum().item()    #/label.size()[0]\n",
    "            sum_acc=sum_acc+acc\n",
    "            if(count%200==0): print('epoch',epoch+1,'/',epochs,'running_loss:',(loss_running/count_batch),'Accuracy',(sum_acc*100/count_batch))\n",
    "\n",
    "        check_loss=(loss_running/count_batch)\n",
    "        lr_scheduler_.step(check_loss)\n",
    "\n",
    "train_function(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running_loss: 0.01664876247756183 Accuracy 64.921875\n"
     ]
    }
   ],
   "source": [
    "#torch.set_printoptions(precision=2)\n",
    "def test_function(test_loader):\n",
    "    loss_running=0\n",
    "    count=0\n",
    "    count_batch=0\n",
    "    sum_acc=0\n",
    "\n",
    "    for input,label in iter(test_loader):\n",
    "        model.eval()\n",
    "\n",
    "        #input.resize_(input.size()[0], 784)   take to forward\n",
    "        y_pred=model(input)   #this is 64 (bacth_size)*10\n",
    "\n",
    "        #if(count==0): print(y_pred.size(),label.size())\n",
    "        loss=criterion(y_pred,label)    #criterion(y_pred,label), crossentropy criterion need long (output of forward) and normal tensor (target)\n",
    "        loss_running=loss_running+loss.item()\n",
    "        count=count+1\n",
    "\n",
    "        count_batch=count_batch+(label.size()[0])\n",
    "        _,y_pred_=torch.max(y_pred,dim=1)    #argmax is the second value returned by torch.max()  ,this collapse dimension to batch size with argmax of probabililty/value (second) item, first one is the value itself \n",
    "\n",
    "        acc=(label==y_pred_).sum().item()    #/label.size()[0]\n",
    "        sum_acc=sum_acc+acc\n",
    "        if(count%100==0): print('running_loss:',(loss_running/count_batch),'Accuracy',(sum_acc*100/count_batch))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_function(test_loader)   # w/o convnet :result show over fitting on train 64%, but overall 50% on test is not too bad wihtout convnet\n",
    "#w/ convnet just after 5 epoch training the test got to 63% (50% train)\n",
    "#after 10 epoch test get to 66% pretty good without big networks\n",
    "\n",
    "#CiFAR10 with inception better in training improve to 64% but not much better for test at 64% (may be need derop out and more fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-3aaf935e6aec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
