cross entroy try out (logoftmax+NNLoss)   log(y_hat)~loss(x,class)=-Log(exp(x[class])/sum(x_class))   , sum across all class values softtmax, log -> log softmax
D(y_hat,y)=-y*log(y_hat)
class:y (more like one hot coded) with multiplied

Y = Variable(torch.LongTensor([0]), requires_grad=False)   Y number of class -1     , max value 2, from 0 ,1,2 (3 class)
Y_pred1 = Variable(torch.Tensor([[2.0, 1.0, 0.1]]))
Y_pred2 = Variable(torch.Tensor([[1.0, 0.1, 2.0]]))
l1 = loss(Y_pred1, Y)
l2 = loss(Y_pred2, Y)
print("PyTorch Loss1 = ", l1.data, 
    "\nPyTorch Loss2=", l2.data)
PyTorch Loss1 =  tensor(0.4170) 
PyTorch Loss2= tensor(1.4170)

if Y=1
PyTorch Loss1 =  tensor(1.4170) 
PyTorch Loss2= tensor(2.3170)

if Y=2
PyTorch Loss1 =  tensor(2.3170) 
PyTorch Loss2= tensor(0.4170)

loss = torch.nn.CrossEntropyLoss(reduce=False)  #to show rensor for each batch sample

Y_pred1 = Variable(torch.Tensor([[0.1, 0.2, 0.7],
                                [0.7, 0.1, 0.2],
                                [0.2, 0.7, 0.1],
                                [0.2, 0.7, 0.1]]))

Y_pred2 = Variable(torch.Tensor([[0.2, 0.7, 0.1],
                                [0.1, 0.7, 0.2],
                                [0.1, 0.2, 0.7],
                                [0.1, 0.2, 0.7]]))
                                
l1 = loss(Y_pred1, Y)
l2 = loss(Y_pred2, Y)

Batch Loss1 =  tensor([ 0.7679,  0.7679,  0.7679,  0.7679]) 
Batch Loss2= tensor([ 1.3679,  1.3679,  1.2679,  1.2679])

np.log(np.exp(0.7)/(np.exp(0.1)+np.exp(0.2)+np.exp(0.7)))
-0.7679495489036249

np.log(np.exp(0.2)/(np.exp(0.1)+np.exp(0.2)+np.exp(0.7)))
-1.267949548903625

np.log(np.exp(0.1)/(np.exp(0.1)+np.exp(0.2)+np.exp(0.7)))
-1.3679495489036249
