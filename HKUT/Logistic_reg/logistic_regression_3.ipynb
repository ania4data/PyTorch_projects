{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data=Variable(torch.tensor([[1.0],[2.0],[3.0],[4.0]]))  # this makes is 3*1,  only one feature, require grad default to false\n",
    "y_data=Variable(torch.tensor([[0.0],[0.0],[1.0],[1.0]]))   #Variable:Wraps a tensor and records the operations applied to it\n",
    "epochs=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.max(y_data, 1)[0]\n",
    "#y_data.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model,self).__init__()   #call in the init of the nn.Module\n",
    "        self.fc1=torch.nn.Linear(1,1)  #one feature in, one feature out 1*1\n",
    "        #self.relu1=torch.nn.ReLU()\n",
    "        #self.tanh1 = torch.nn.Tanh()\n",
    "        \n",
    "        \n",
    "    def forward(self,x):  \n",
    "        \n",
    "        #y_pred=F.sigmoid(self.fc1(x))\n",
    "        #t=self.tanh1(self.fc1(x))\n",
    "        t=self.fc1(x)\n",
    "        #t=F.tanh(t)     #in order to use tanh since it keep the out put between -1 and 1, need to modify ytarget to -1 and 1\n",
    "                        # also the binary BCF loss would not work for -1 and 1 (need 0,1 ) so had to change loss function to\n",
    "                        # torch.nn.SoftMarginLoss target tensor y (containing 1 or -1)\n",
    "        t=F.relu(t)\n",
    "        #t=torch.nn.Tanh(t)\n",
    "        #t=self.relu1(t)\n",
    "        #t=F.softmax(t,dim=0)\n",
    "        #y_pred=self.soft1(t)\n",
    "        y_pred=F.softmax(t,dim=0)\n",
    "        \n",
    "        return y_pred\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of Model(\n",
      "  (fc1): Linear(in_features=1, out_features=1, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1805])\n",
      "tensor(0.9578)\n"
     ]
    }
   ],
   "source": [
    "for parameter in model.parameters(): print((parameter[0]))   #initillizing it on its own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.1805]]), Parameter containing:\n",
       " tensor([ 0.9578])]"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list((model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=torch.nn.BCELoss(size_average=True)   #loss initilization, binary cross entropy need to be 0,1 y\n",
    "#criterion=torch.nn.HingeEmbeddingLoss(margin=0.0)  #soso\n",
    "#criterion=torch.nn.SoftMarginLoss()\n",
    "#criterion=torch.nn.CrossEntropyLoss()   #cross entropy and NLL donot accept onehot code need (input*C) tensor for target (long()?)\n",
    "#criterion=torch.nn.NLLLoss()\n",
    "#optimizer=torch.optim.SGD(model.parameters(),lr=0.01)  #init SGD with initilized model.paramter  (w/ grad, weight and bias)\n",
    "#optimizer=torch.optim.Adam(model.parameters(),lr=0.1)\n",
    "#optimizer=torch.optim.SGD(model.parameters(),lr=0.01,momentum=0.9)\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=0.1)\n",
    "\n",
    "#torch.optim will hold the current state and will update the parameters based on the computed gradients. iterable containing the parameters (Variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: tensor(0.9684) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 10 loss: tensor(0.6254) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 20 loss: tensor(0.5403) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 30 loss: tensor(0.5139) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 40 loss: tensor(0.5040) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 50 loss: tensor(0.4998) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 60 loss: tensor(0.4980) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 70 loss: tensor(0.4971) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 80 loss: tensor(0.4967) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 90 loss: tensor(0.4965) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 100 loss: tensor(0.4963) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 110 loss: tensor(0.4963) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 120 loss: tensor(0.4963) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 130 loss: tensor(0.4963) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 140 loss: tensor(0.4963) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 150 loss: tensor(0.4962) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 160 loss: tensor(0.4962) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 170 loss: tensor(0.4962) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 180 loss: tensor(0.4962) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 190 loss: tensor(0.4962) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 200 loss: tensor(0.4962) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 210 loss: tensor(0.4962) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 220 loss: tensor(0.4962) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 230 loss: tensor(0.4962) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 240 loss: tensor(0.4962) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 250 loss: tensor(0.4962) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 260 loss: tensor(0.4962) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 270 loss: tensor(0.4962) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 280 loss: tensor(0.4962) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 290 loss: tensor(0.4962) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 300 loss: tensor(0.4962) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 310 loss: tensor(0.4962) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 320 loss: tensor(0.4962) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 330 loss: tensor(0.4962) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 340 loss: tensor(0.4962) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 350 loss: tensor(0.4962) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 360 loss: tensor(0.4962) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 370 loss: tensor(0.4962) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 380 loss: tensor(0.4962) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 390 loss: tensor(0.4962) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 400 loss: tensor(0.4962) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 410 loss: tensor(0.4962) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 420 loss: tensor(0.4962) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 430 loss: tensor(0.4962) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 440 loss: tensor(0.4962) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 450 loss: tensor(0.4962) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 460 loss: tensor(0.4962) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 470 loss: tensor(0.4962) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 480 loss: tensor(0.4962) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n",
      "epoch: 490 loss: tensor(0.4962) 1.0 tensor([ 1.]) 4.0 tensor([ 1.]) 7.0 tensor([ 1.])\n"
     ]
    }
   ],
   "source": [
    "loss_=torch.tensor([])\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    #for parameter in model.parameters(): print('epoch',epoch,parameter[0]) \n",
    "    y_pred=model(x_data)   #this automtically call forward\n",
    "    #target.squeeze(1)\n",
    "    #target=y_data.squeeze(1)\n",
    "    target=y_data          #.view(-1)\n",
    "    loss=criterion(y_pred,target)\n",
    "    \n",
    "    \n",
    "    tmp=torch.tensor([loss])\n",
    "    loss_=torch.cat((loss_,tmp))\n",
    "    optimizer.zero_grad()   #initilize gradient to zero before every step, to avoid accumulation ,zero off good for RNN\n",
    "    loss.backward()         #calculate gradient\n",
    "    optimizer.step()        #update parameter (weights)\n",
    "    t0=torch.tensor([1.0])\n",
    "    t1=torch.tensor([4.0])\n",
    "    t2=torch.tensor([7.0])\n",
    "    output0=model.forward(t0)\n",
    "    output1=model.forward(t1)\n",
    "    output2=model.forward(t2)\n",
    "    #_, predicted0=torch.max(output0.data, 1)\n",
    "    #_, predicted1=torch.max(output1.data, 1)\n",
    "    #_, predicted2=torch.max(output2.data, 1)\n",
    "    if(epoch%10==0): print('epoch:',epoch,'loss:',loss,'1.0',output0,'4.0',output1,'7.0',output2)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.1656])\n",
      "tensor(0.9578)\n"
     ]
    }
   ],
   "source": [
    "for i,parameter in enumerate(model.parameters()):\n",
    "    #print('w:',list(model.parameters()[0]),'bias:',list(model.parameters()[0])) \n",
    "\n",
    "    print(parameter[0])    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_softmax_relu=loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#plt.plot(np.arange(0,epochs,1),loss_SGD_01.detach().numpy(),label='SGD_0.1_sigmoid')  #lr=0.01 #need to detach gradient for graph breakdown otherwise won't do numpy in the middle of graph (good for end of run)\n",
    "plt.plot(np.arange(0,epochs,1),loss_softmax_relu.detach().numpy(),label='SGD_softmax_relu') \n",
    "#plt.plot(np.arange(0,epochs,1),loss_sgd_mom.detach().numpy(),label='SGD_0.01_mom_0.9')   #lr=0.01 momentum=0.9\n",
    "#plt.plot(np.arange(0,epochs,1),loss_Adam.detach().numpy(),label='Adam_0.1') #lr=0.1\n",
    "#plt.xlim([-1,40])\n",
    "plt.legend()\n",
    "plt.show() \n",
    "\n",
    "\n",
    "#SGD w/o momentum look better in converging, than with momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6775)"
      ]
     },
     "execution_count": 732,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=torch.tensor([4.0])\n",
    "model.forward(test).data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
