{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/eBRPvWB.png)\n",
    "\n",
    "# Practical PyTorch: Generating Shakespeare with a Character-Level RNN\n",
    "\n",
    "[In the RNN classification tutorial](https://github.com/spro/practical-pytorch/blob/master/char-rnn-classification/char-rnn-classification.ipynb) we used a RNN to classify text one character at a time. This time we'll generate text one character at a time.\n",
    "\n",
    "```\n",
    "> python generate.py -n 500\n",
    "\n",
    "PAOLTREDN:\n",
    "Let, yil exter shis owrach we so sain, fleas,\n",
    "Be wast the shall deas, puty sonse my sheete.\n",
    "\n",
    "BAUFIO:\n",
    "Sirh carrow out with the knonuot my comest sifard queences\n",
    "O all a man unterd.\n",
    "\n",
    "PROMENSJO:\n",
    "Ay, I to Heron, I sack, againous; bepear, Butch,\n",
    "An as shalp will of that seal think.\n",
    "\n",
    "NUKINUS:\n",
    "And house it to thee word off hee:\n",
    "And thou charrota the son hange of that shall denthand\n",
    "For the say hor you are of I folles muth me?\n",
    "```\n",
    "\n",
    "This one might make you question the series title &mdash; \"is that really practical?\" However, these sorts of generative models form the basis of machine translation, image captioning, question answering and more. See the [Sequence to Sequence Translation tutorial](https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb) for more on that topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommended Reading\n",
    "\n",
    "I assume you have at least installed PyTorch, know Python, and understand Tensors:\n",
    "\n",
    "* http://pytorch.org/ For installation instructions\n",
    "* [Deep Learning with PyTorch: A 60-minute Blitz](https://github.com/pytorch/tutorials/blob/master/Deep%20Learning%20with%20PyTorch.ipynb) to get started with PyTorch in general\n",
    "* [jcjohnson's PyTorch examples](https://github.com/jcjohnson/pytorch-examples) for an in depth overview\n",
    "* [Introduction to PyTorch for former Torchies](https://github.com/pytorch/tutorials/blob/master/Introduction%20to%20PyTorch%20for%20former%20Torchies.ipynb) if you are former Lua Torch user\n",
    "\n",
    "It would also be useful to know about RNNs and how they work:\n",
    "\n",
    "* [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) shows a bunch of real life examples\n",
    "* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) is about LSTMs specifically but also informative about RNNs in general\n",
    "\n",
    "Also see these related tutorials from the series:\n",
    "\n",
    "* [Classifying Names with a Character-Level RNN](https://github.com/spro/practical-pytorch/blob/master/char-rnn-classification/char-rnn-classification.ipynb) uses an RNN for classification\n",
    "* [Generating Names with a Conditional Character-Level RNN](https://github.com/spro/practical-pytorch/blob/master/conditional-char-rnn/conditional-char-rnn.ipynb) builds on this model to add a category as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data\n",
    "\n",
    "The file we are using is a plain text file. We turn any potential unicode characters into plain ASCII by using the `unidecode` package (which you can install via `pip` or `conda`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 2046653\n",
      "81\n",
      "['\\t', '\\n', ' ', '!', '\"', '&', \"'\", '(', ')', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "\n",
    "\n",
    "#all_characters = string.printable\n",
    "#n_characters = len(all_characters)\n",
    "#print(n_characters)\n",
    "#print(all_characters)\n",
    "file = unidecode.unidecode(open('sherlock_holmes_all.txt').read())  #unicode to ascii\n",
    "#file = unidecode.unidecode(open('shakespeare.txt').read())  #unicode to ascii\n",
    "#text = open(f'{PATH}nietzsche.txt').read()\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)  #basically nuumber of characters\n",
    "#print(file) #print whole file\n",
    "\n",
    "#all_characters = string.printable\n",
    "#n_characters = len(all_characters)\n",
    "#print(n_characters)\n",
    "#print(all_characters)\n",
    "all_characters = sorted(list(set(file)))\n",
    "n_characters = len(all_characters)\n",
    "print(n_characters)\n",
    "print(all_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\t\\t\\t\\t\\tTHE ADVENTURES OF SHERLOCK HOLMES\\n\\n\\t\\t\\t\\t\\t\\t   Arthur Conan Doyle\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\tTable of contents\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_characters.insert(0, \"\\0\")\n",
    "\n",
    "# ''.join(all_characters[1:-6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make inputs out of this big string of data, we will be splitting it into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts of the little town, finally returning to the hotel, where I\n",
      " lay upon the sofa and tried to interest myself in a yellow-backed\n",
      " novel. The puny plot of the story was so thin, however, when compared\n",
      " to the deep mystery through which we were groping, and I found my\n",
      " attention wander so continually from the action to the fact, that I\n",
      " at last flung it across the room and gave myself up entirely to a\n",
      " consideration of the events of the day. Supposing that this unhappy\n",
      " young man's story were abso\n"
     ]
    }
   ],
   "source": [
    "chunk_len = 500\n",
    "\n",
    "def random_chunk():\n",
    "    start_index = random.randint(0, file_len - chunk_len)  #start index\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return file[start_index:end_index]\n",
    "\n",
    "print(random_chunk())  #give a 500 length sentence randomly each time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Model\n",
    "\n",
    "This model will take as input the character for step $t_{-1}$ and is expected to output the next character $t$. There are three layers - one linear layer that encodes the input character into an internal state, one GRU layer (which may itself have multiple layers) that operates on that internal state and a hidden state, and a decoder layer that outputs the probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.encoder_ = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,dropout=0.5)\n",
    "        #self.dropout2=nn.Dropout2d()\n",
    "        self.decoder_ = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input_, hidden):\n",
    "        input_=input_.to(device)\n",
    "        input_ = self.encoder_(input_.view(1, -1))\n",
    "        output, hidden = self.gru(input_.view(1, 1, -1), hidden)\n",
    "        #output=self.dropout2(output)\n",
    "        output = self.decoder_(output.view(1, -1))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs and Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each chunk will be turned into a tensor, specifically a `LongTensor` (used for integer values), by looping through the characters of the string and looking up the index of each character in `all_characters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 55,  56,  57,  29,  30,  31])\n"
     ]
    }
   ],
   "source": [
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return Variable(tensor)\n",
    "\n",
    "print(char_tensor('abcDEF'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can assemble a pair of input and target tensors for training, from a random chunk. The input will be all characters *up to the last*, and the target will be all characters *from the first*. So if our chunk is \"abc\" the input will correspond to \"ab\" while the target is \"bc\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 76,  59,  72,   2,  74,  62,  59,   2,  61,  72,  59,  55,\n",
      "         74,   2,  57,  63,  74,  79,  11,   1,   2,  44,  62,  59,\n",
      "         72,  66,  69,  57,  65,   2,  33,  69,  66,  67,  59,  73,\n",
      "          2,  77,  55,  73,   2,  55,  66,  72,  59,  55,  58,  79,\n",
      "          2,  55,  74,   2,  56,  72,  59,  55,  65,  60,  55,  73,\n",
      "         74,   2,  77,  62,  59,  68,   2,  34,   2,  57,  55,  67,\n",
      "         59,   2,  58,  69,  77,  68,  11,   1,   1,   2,   4,  50,\n",
      "         69,  75,   2,  77,  63,  66,  66,   2,  59,  78,  57,  75,\n",
      "         73,  59,   2,  67,  59,   2,  60,  69,  72,   2,  68,  69,\n",
      "         74,   2,  77,  55,  63,  74,  63,  68,  61,   2,  60,  69,\n",
      "         72,   2,  79,  69,  75,   9,   4,   2,  73,  55,  63,  58,\n",
      "          2,  62,  59,  24,   2,   4,  34,   2,  62,  55,  76,  59,\n",
      "          9,   2,  34,   1,   2,  60,  69,  72,  59,  73,  59,  59,\n",
      "          9,   2,  55,   2,  76,  59,  72,  79,   2,  56,  75,  73,\n",
      "         79,   2,  58,  55,  79,   2,  56,  59,  60,  69,  72,  59,\n",
      "          2,  67,  59,   2,  63,  68,   2,  66,  69,  69,  65,  63,\n",
      "         68,  61,   2,  63,  68,  74,  69,   2,  74,  62,  63,  73,\n",
      "          2,  57,  55,  73,  59,   2,  69,  60,   2,  79,  69,  75,\n",
      "         68,  61,   1,   2,  40,  70,  59,  68,  73,  62,  55,  77,\n",
      "          6,  73,  11,   4,   1,   1,   2,   4,  48,  62,  55,  74,\n",
      "          2,  73,  74,  59,  70,  73,   2,  77,  63,  66,  66,   2,\n",
      "         79,  69,  75,   2,  74,  55,  65,  59,  25,   4,   2,  34,\n",
      "          2,  55,  73,  65,  59,  58,  11,   1,   1,   2,   4,  34,\n",
      "         74,   2,  77,  63,  66,  66,   2,  76,  59,  72,  79,   2,\n",
      "         67,  75,  57,  62,   2,  58,  59,  70,  59,  68,  58,   2,\n",
      "         75,  70,  69,  68,   2,  74,  62,  59,   2,  72,  59,  73,\n",
      "         75,  66,  74,  73,   2,  69,  60,   2,  67,  79,   2,  60,\n",
      "         63,  72,  73,  74,   2,  63,  68,  71,  75,  63,  72,  63,\n",
      "         59,  73,  11,   2,  34,   1,   2,  67,  55,  79,   2,  62,\n",
      "         55,  76,  59,   2,  74,  69,   2,  61,  69,   2,  58,  69,\n",
      "         77,  68,   2,  74,  69,   2,  33,  69,  72,  73,  62,  55,\n",
      "         67,   9,   2,  55,  60,  74,  59,  72,   2,  55,  66,  66,\n",
      "         11,   4,   1,   1,   2,   4,  50,  69,  75,   2,  77,  63,\n",
      "         66,  66,   2,  68,  69,  74,   2,  61,  69,   2,  74,  62,\n",
      "         59,  72,  59,   2,  60,  63,  72,  73,  74,  25,   4,   1,\n",
      "          1,   2,   4,  39,  69,   9,   2,  34,   2,  73,  62,  55,\n",
      "         66,  66,   2,  57,  69,  67,  67,  59,  68,  57,  59,   2,\n",
      "         77,  63,  74,  62,   2,  74,  62,  59,   2,  28,  63,  74,\n",
      "         79,  11,   2,  35,  75,  73,  74,   2,  72,  63,  68,  61,\n",
      "          2,  74,  62,  59,   2,  56,  59,  66,  66,   2,  55,  68,\n",
      "         58,   2,  74,  62,  59,   2,  67,  55,  63,  58,   1,   2,\n",
      "         77,  63,  66,  66,   2,  56,  72,  63]), tensor([ 59,  72,   2,  74,  62,  59,   2,  61,  72,  59,  55,  74,\n",
      "          2,  57,  63,  74,  79,  11,   1,   2,  44,  62,  59,  72,\n",
      "         66,  69,  57,  65,   2,  33,  69,  66,  67,  59,  73,   2,\n",
      "         77,  55,  73,   2,  55,  66,  72,  59,  55,  58,  79,   2,\n",
      "         55,  74,   2,  56,  72,  59,  55,  65,  60,  55,  73,  74,\n",
      "          2,  77,  62,  59,  68,   2,  34,   2,  57,  55,  67,  59,\n",
      "          2,  58,  69,  77,  68,  11,   1,   1,   2,   4,  50,  69,\n",
      "         75,   2,  77,  63,  66,  66,   2,  59,  78,  57,  75,  73,\n",
      "         59,   2,  67,  59,   2,  60,  69,  72,   2,  68,  69,  74,\n",
      "          2,  77,  55,  63,  74,  63,  68,  61,   2,  60,  69,  72,\n",
      "          2,  79,  69,  75,   9,   4,   2,  73,  55,  63,  58,   2,\n",
      "         62,  59,  24,   2,   4,  34,   2,  62,  55,  76,  59,   9,\n",
      "          2,  34,   1,   2,  60,  69,  72,  59,  73,  59,  59,   9,\n",
      "          2,  55,   2,  76,  59,  72,  79,   2,  56,  75,  73,  79,\n",
      "          2,  58,  55,  79,   2,  56,  59,  60,  69,  72,  59,   2,\n",
      "         67,  59,   2,  63,  68,   2,  66,  69,  69,  65,  63,  68,\n",
      "         61,   2,  63,  68,  74,  69,   2,  74,  62,  63,  73,   2,\n",
      "         57,  55,  73,  59,   2,  69,  60,   2,  79,  69,  75,  68,\n",
      "         61,   1,   2,  40,  70,  59,  68,  73,  62,  55,  77,   6,\n",
      "         73,  11,   4,   1,   1,   2,   4,  48,  62,  55,  74,   2,\n",
      "         73,  74,  59,  70,  73,   2,  77,  63,  66,  66,   2,  79,\n",
      "         69,  75,   2,  74,  55,  65,  59,  25,   4,   2,  34,   2,\n",
      "         55,  73,  65,  59,  58,  11,   1,   1,   2,   4,  34,  74,\n",
      "          2,  77,  63,  66,  66,   2,  76,  59,  72,  79,   2,  67,\n",
      "         75,  57,  62,   2,  58,  59,  70,  59,  68,  58,   2,  75,\n",
      "         70,  69,  68,   2,  74,  62,  59,   2,  72,  59,  73,  75,\n",
      "         66,  74,  73,   2,  69,  60,   2,  67,  79,   2,  60,  63,\n",
      "         72,  73,  74,   2,  63,  68,  71,  75,  63,  72,  63,  59,\n",
      "         73,  11,   2,  34,   1,   2,  67,  55,  79,   2,  62,  55,\n",
      "         76,  59,   2,  74,  69,   2,  61,  69,   2,  58,  69,  77,\n",
      "         68,   2,  74,  69,   2,  33,  69,  72,  73,  62,  55,  67,\n",
      "          9,   2,  55,  60,  74,  59,  72,   2,  55,  66,  66,  11,\n",
      "          4,   1,   1,   2,   4,  50,  69,  75,   2,  77,  63,  66,\n",
      "         66,   2,  68,  69,  74,   2,  61,  69,   2,  74,  62,  59,\n",
      "         72,  59,   2,  60,  63,  72,  73,  74,  25,   4,   1,   1,\n",
      "          2,   4,  39,  69,   9,   2,  34,   2,  73,  62,  55,  66,\n",
      "         66,   2,  57,  69,  67,  67,  59,  68,  57,  59,   2,  77,\n",
      "         63,  74,  62,   2,  74,  62,  59,   2,  28,  63,  74,  79,\n",
      "         11,   2,  35,  75,  73,  74,   2,  72,  63,  68,  61,   2,\n",
      "         74,  62,  59,   2,  56,  59,  66,  66,   2,  55,  68,  58,\n",
      "          2,  74,  62,  59,   2,  67,  55,  63,  58,   1,   2,  77,\n",
      "         63,  66,  66,   2,  56,  72,  63,  68]))\n"
     ]
    }
   ],
   "source": [
    "def random_training_set():    \n",
    "    chunk = random_chunk()\n",
    "    inp = char_tensor(chunk[:-1])\n",
    "    target = char_tensor(chunk[1:])\n",
    "   # target=torch.LongTensor([target])\n",
    "    return inp, target\n",
    "\n",
    "print(random_training_set())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating\n",
    "\n",
    "To evaluate the network we will feed one character at a time, use the outputs of the network as a probability distribution for the next character, and repeat. To start generation we pass a priming string to start building up the hidden state, from which we then generate one character at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
    "    decoder.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        hidden = decoder.init_hidden()\n",
    "        prime_input = char_tensor(prime_str)\n",
    "        predicted = prime_str\n",
    "\n",
    "        # Use priming string to \"build up\" hidden state\n",
    "        for p in range(len(prime_str) - 1):\n",
    "            _, hidden = decoder(prime_input[p], hidden)\n",
    "        inp = prime_input[-1]\n",
    "\n",
    "        for p in range(predict_len):\n",
    "            output, hidden = decoder(inp, hidden)\n",
    "\n",
    "            # Sample from the network as a multinomial distribution\n",
    "            output_dist = output.data.view(-1).div(temperature).exp()\n",
    "            top_i = torch.multinomial(output_dist, 1)[0]\n",
    "\n",
    "            # Add predicted character to string and use as next input\n",
    "            predicted_char = all_characters[top_i]\n",
    "            predicted += predicted_char\n",
    "            inp = char_tensor(predicted_char)\n",
    "    decoder.train()        \n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helper to print the amount of time passed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, math\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(inp, target):\n",
    "    hidden = decoder.init_hidden()\n",
    "    decoder.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "\n",
    "    for c in range(chunk_len):\n",
    "        output, hidden = decoder(inp[c], hidden)\n",
    "        #output = output.unsqueeze(0)\n",
    "        #target[c]=torch.LongTensor([target[c]])\n",
    "        target_=torch.tensor([target[c]]).to(device)\n",
    "        loss += criterion(output, target_)\n",
    "\n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "   # print(loss)\n",
    "\n",
    "\n",
    "    return loss.data / chunk_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the training parameters, instantiate the model, and start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12m 47s (1000 3%) 1.6627]\n",
      "Home, and fear you have lirge our onger, sime the farted it which come, you was could\n",
      " a manes.\"\n",
      "\n",
      " \"We do simy eve no would the head. Leasty, and man this light the is that me. The being and you\n",
      " alsered but roum in he canthing of can this and was heard of the duther. It in the nevery gornoms,\" so were me the dave and hadk dy to dears wometed to the pisitable-so him you that as my clad a\n",
      " dacked you \n",
      "\n",
      "[25m 44s (2000 6%) 1.8550]\n",
      "Hover.\n",
      "\n",
      " \"And the poils of the me. Dows if I parctod and were had be the sase him person in his\n",
      " known.  Mr. Holmes, I have can and that,\" shom the lught. I langed\n",
      " the doon. Now even such a knowing any recristlement linite. I arred, door of the long and\n",
      " an alsome stranger. But I surpreated a looks her\n",
      " recrambed in his decorcull notked to exciping of occoursely lay of remark his being,\n",
      " and not he \n",
      "\n",
      "[40m 11s (3000 10%) 1.7199]\n",
      "Hould enough good to sid. They my ask in the\n",
      " consaid, that he had been have bidy we was before the birded from her\n",
      " fear. Surprisally not it have the what is round that shall rather a should\n",
      " chain to-minch of side and would be upention in the carrow. It was with\n",
      " the really with which he man could back interest I had been\n",
      " combing must have sell. He had do not a see glounting for be in the face up \n",
      "\n",
      "[54m 58s (4000 13%) 1.5495]\n",
      "Hould redort of his world, and he was a\n",
      " some hinding the hope, and I supposes by a firning a strang which say, as it is a\n",
      " leading for the begred that his own of un a upprowbent who be the look did not been to be his\n",
      " window, and it was yessily all the strock at moment as something his\n",
      " for the sirely and and a lacking man be too pale, and it about\n",
      " of a digning down all his case in the fact, and i \n",
      "\n",
      "[69m 54s (5000 16%) 1.5136]\n",
      "Hould never the senmens nature, in\n",
      " upon him news my cound to me from him\n",
      " to of the fitted possible him of me. He had begorth had been had found my\n",
      " on, for my shall seen the face from him.  I will asked him I said a feen;\n",
      " with the pton that he should has look for life. Only expect?\"\n",
      "\n",
      " \"A return and the son horranance would for the experise in excellent of could find here, and any of know.\n",
      "\n",
      " \"A fi \n",
      "\n",
      "[83m 50s (6000 20%) 1.5016]\n",
      "How have eventon and had the stroy\n",
      " down to be minut all more of a several obvicable had a stail and\n",
      " might no doctor, thought that she had left me the friend of the strange of their taken face--that would a man is\n",
      " nothing that I chazing a looks compon the strate, and the first scan in the\n",
      " Barders of the did not fellow little before the man had seems memer\n",
      " had now, I am had promose about it was t \n",
      "\n",
      "[97m 20s (7000 23%) 1.4542]\n",
      "Horrical started before\n",
      " were are one of much I such us, and I recovered to be the corner of his known to me the\n",
      " strangled to be to a passive after then something thrad. I\n",
      " last a husbands of that such a villice to the matters in chourse of A\n",
      "r\t\t\tb2\t0 starting scotters. \"It is any about that it I have not a lotten to be at the assured we culoned-room.\"\n",
      "\n",
      " \"I from Let as he had a crieds colones. It w \n",
      "\n",
      "[110m 40s (8000 26%) 1.4960]\n",
      "Holmes, looking, when the\n",
      " house ewerest that he may be recagely that he is vining faintly be a later of a\n",
      " some mouthing in his sat in the minstrem complection weak the first from the\n",
      " house of a wear from my ching, and but flowed his real in his wains.\"\n",
      "\n",
      " \"Whan dark a man port of smile into the two been friend of his fellow of showpor,\"\n",
      " said Holmes. \"I think it could not have no signing me liven  \n",
      "\n",
      "[124m 22s (9000 30%) 1.5903]\n",
      "Holmes, about its, which had\n",
      " better that you are from a horse to him be a sat, and that he is\n",
      " not my profection's know, but that a spring him that the stenged of\n",
      " the and spank to case how fell to shangely threated from the tell\n",
      " of the Crack and behand of the young for that I first the\n",
      " lips of the other hand. In danger why were the capes befuling by the traily could not\n",
      " my little may be a quest \n",
      "\n",
      "[139m 9s (10000 33%) 1.5477]\n",
      "Hour easy upon she is long\n",
      " seafed that he found it, and the great stated I friends on to\n",
      " be strunged her fuest--a calor that he asked that it was a second two good\n",
      " once enough, he I whom I should able that I see as this play was save\n",
      " by my girns and his pipical. It is these was to have so far no\n",
      " something to the after the descontine of a dresss. I were I have been\n",
      " some weild sleoing and reishe \n",
      "\n",
      "[154m 9s (11000 36%) 1.5378]\n",
      "Ho; he pay in a head, and\n",
      " the table was a country into the skilleh for his corner would\n",
      " be from the cab? I'll be whan she was away that the most man faced, and that he\n",
      " came once all world me, and a letter of the started are fashion which\n",
      " was there wished and foring you, then?\"\n",
      "\n",
      " \"I thought in friend had not a friend that, there is a way.\"\n",
      "\n",
      " \"No only senied broken, and I would be a lead of the de \n",
      "\n",
      "[169m 5s (12000 40%) 1.6091]\n",
      "Hourness and heart\n",
      " of the singular. He thought they consusting some stops. She like now\n",
      " of the reight.\"\n",
      "\n",
      " \"'Yes, sir, sightly away. Why seem, that you entered it seemed any of a trains. Yes,\n",
      " this is a evidem, and the men his night as I look could not paper which be a\n",
      " struck in some crime.\"\n",
      "\n",
      " \"It is a quarter, Did, the book continued out of the lamp.\"\n",
      "\n",
      " \"They are no little with you that he had on \n",
      "\n",
      "[183m 58s (13000 43%) 1.4781]\n",
      "How the man considellity.\n",
      "\n",
      " \"And you should see fitting a little pick of it. I have some sast can braking to the\n",
      " voice upon the refar. It is an infacifores and put when I could my legent which he\n",
      " had heard or him, and the window of course, do.\"\n",
      "\n",
      " I had nothing as we have not has seat, we should not considering\n",
      " them.\"\n",
      "\n",
      " \"What caught the known, though their bed year of simple to you there,\n",
      " who see \n",
      "\n",
      "[198m 49s (14000 46%) 1.4281]\n",
      "However was her confess of it is treaty the first.\n",
      " That were could not be stood upon every, you think o'clock ship\n",
      " with the orferban Street & The man, too married that this ortrite carry to him a\n",
      " very hand, boy which had not to see it. You know that you are a to be\n",
      " in the road to have scread that you have frenged in those componion which\n",
      " was the carrying which looked of my country. I may be not \n",
      "\n",
      "[213m 42s (15000 50%) 1.4477]\n",
      "How as the brain of Sherlicker for the tlight sprop\n",
      " out in him, and dreak this all at the spudy has works to the door.  A friend in his own\n",
      " that I found away his whole own that our suddenly been smoked up to see the house with\n",
      " the case and room that all to the death had observed to her about\n",
      " my passion, were over a man, of the sides of the conclueance is mind\n",
      " Miss Seasion teared the during, and \n",
      "\n",
      "[228m 32s (16000 53%) 1.4331]\n",
      "How passed\n",
      " to Carried the black in adventhers for the driven which as I pass\n",
      " the bust was serve were which she said some drawn which I was a enbery and\n",
      " the returned, and her very stuiney took not behindly the friend and\n",
      " the stranger--my foreing he had not the sound and word his jounds and\n",
      " turned to the traction of that he had he saw a man to this racters over\n",
      " that a was an room his face. At ha \n",
      "\n",
      "[243m 17s (17000 56%) 1.4326]\n",
      "How an absolutely and like the\n",
      " vanial into chosition. Why, my compan of the intrudent\n",
      " chair upon the facts man. Then there is one of instance in the\n",
      " vicar from that the face of the wripted which came that I\n",
      " client two of the voice of the third which the chair of a man see\n",
      " so danger, the strange floors could felling to\n",
      " the ideen with when I did his spangic with a\n",
      " flutters which was impossible  \n",
      "\n",
      "[258m 12s (18000 60%) 1.3433]\n",
      "How a mind and an\n",
      " instate for the son of the fortunation as my own yectod between a clam, but it is passed with a\n",
      " will end for his an experate I have really and locked find him\n",
      " to go something. There went upon my family and that open Holmes\n",
      " made since herself to companion. The room I full on the door had read on\n",
      " the hagged our words. It had then was a long see to the door\n",
      " in the restign. Now,  \n",
      "\n",
      "[272m 49s (19000 63%) 1.4112]\n",
      "Holmes might\n",
      " may was popering us like the companions. The door sofried to destrayed. \n",
      " The paper that you might have even mile of the end each do your man bis\n",
      " your hall windows of many passion? \"I may be so man in a come\n",
      " minutes of the fortunation on the kuy with a daught through my\n",
      " hand. They have the atting, in Barter.\n",
      "\n",
      " \"I don't see you with his person in the man and\n",
      " life for one which had s \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[286m 41s (20000 66%) 1.4256]\n",
      "However, I written the room, which\n",
      " had glad in have she could take for the picks of his instant. I found his conscious way to you\n",
      " know the stidd of the presents. That are in a question of\n",
      " the darkless maids. His lal was suddenly knowledge to my known my\n",
      " planed, and it was a burrial upon the whole strong, and he had\n",
      " that there is round the thick and had sudderly slight, had been faint\n",
      " had the f \n",
      "\n",
      "[300m 27s (21000 70%) 1.3925]\n",
      "How my flight--unusual as he\n",
      " should man would companion to state Hildal handly I know that you know a little\n",
      " sound. I can thinks ago every tempers of it. Now were percope the police,\n",
      " and it is the Corner was some expection of surply which I was look estant on the\n",
      " side. There is in this who fell. Then, who saw him, and I cannot every passive in onch\n",
      " head it as I should be the face, and I could e \n",
      "\n",
      "[314m 18s (22000 73%) 1.2775]\n",
      "Hook from this opened action. It was a more. One shown very\n",
      " event and my will some packet manner with me of the exome indeed and\n",
      " oppence. He got the only of his half of the kneas has elsed of\n",
      " our points and all the road time and some quiet which like to yourself with the\n",
      " two day. A should have dared up which he was not a better bedding to him to\n",
      " be a most burning him, and I hat save a house wer \n",
      "\n",
      "[328m 14s (23000 76%) 1.5955]\n",
      "How we had some public\n",
      " with as I was being for a way and left all as his last of the box as the face.\n",
      " When we have right, Therefore-life, but I had been remarked an\n",
      " catraspers of it. Remarked when I was not come upon the Milver\n",
      " door of his seems and a strange presence.\n",
      "\n",
      " \"Drock me, \"I have so had glances to simple over their one of a head of the instant I\n",
      " reason been to desperstand, he had an e \n",
      "\n",
      "[342m 16s (24000 80%) 1.3633]\n",
      "How there is on us it appeared its secret.\n",
      " And we have in some look by difference we clearly of a black finger\n",
      " sading-with him, but our face more she took him against the\n",
      " strong country back and Todded, but he had no done that we have been\n",
      " chested the sencise lired at the window from how we carked to a\n",
      " later an hour, and the chair, and what a stremgl was kept along him\n",
      " that he was an autural c \n",
      "\n",
      "[356m 16s (25000 83%) 1.3975]\n",
      "How Mr. Jose, and\n",
      " I am cordin him to the way been the man.\n",
      "\n",
      " \"Well, then, in a room, which are reswicul to the papers of\n",
      " so cabition which we artered with the crime of his singular noces.\n",
      " My desire event of these was before it had been after, we had gone\n",
      " two side which before he were as your life. I not some wordded\n",
      " understand in then the burning business had been done at the\n",
      " wild brimes. What \n",
      "\n",
      "[370m 4s (26000 86%) 1.5094]\n",
      "Horm there\n",
      " that the passive personal things is their man's knife.'\n",
      "\n",
      " \"What take it returned, who is the crockes face to provided a very friend of\n",
      " the hundicies and words that the stamp came that the fear was fiers. Wy must\n",
      " reach to my live picking from the sour of the way in the pall of his street.\n",
      " His friends and which with a very detarted with a money of the should then the girl in the\n",
      " house? \n",
      "\n",
      "[384m 2s (27000 90%) 1.3482]\n",
      "Holmes difficultty a\n",
      " upon our most broking steps had left his importance for the\n",
      " cases of the vical, and there was excellently the time slepper.\n",
      " Room, thet is never known that the least hums in a lock where it\n",
      " asked at the stair in the consider which had not tried this most face.\n",
      " You are it made the lights, and those accorner was a trainly a point and\n",
      " when It seems to experiently stone for a s \n",
      "\n",
      "[397m 51s (28000 93%) 1.1940]\n",
      "Hook him at the case. It\n",
      " was don't would graw upon the note in the until you've seemed your\n",
      " the Jone. We were no bar.\"\n",
      "\n",
      " \"The room.\"\n",
      "\n",
      " \"My day--\"he will be the woman. Of course I bed, I shall dear it\n",
      " recovered, Mr. Holmes, and not the deed of useful Rook, as they were such\n",
      " the gangly. Do, therefore they had they can possibly anything even with very\n",
      " monome. I could have a paling feel of it.\"\n",
      "\n",
      " \" \n",
      "\n",
      "[411m 48s (29000 96%) 1.7150]\n",
      "How well, for we appeared to the room\n",
      " facing his heavy to a foral and with his through a window, and there may be\n",
      " been known his prodector, and that his singuled by the same and her upon the\n",
      " whide of the tracks which he was about the man which applocked to the one of\n",
      " the connection of your words me to be a chair to the morning, \"I had some\n",
      " mannerful formers to know my few wife in the yampon of  \n",
      "\n",
      "[425m 45s (30000 100%) 1.2732]\n",
      "Holmes was not a single with\n",
      " the singular blind in my bath.  That were seen to some station, however, what have gries to say the cases which has\n",
      " always done until he was over to drawn to see the hand which will sat\n",
      " the room. \"It is a pale in hop a convid the contittable whose active in an\n",
      " in despaper upon the gentleman to my fact. I shall doing it\n",
      " are the instant?\"\n",
      "\n",
      " \"Yes?\"\n",
      "\n",
      " \"It was as to go y \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 30000\n",
    "print_every = 1000\n",
    "plot_every = 1000\n",
    "hidden_size = 200\n",
    "n_layers = 3\n",
    "lr = 0.001\n",
    "\n",
    "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "#decoder_optimizer=torch.optim.RMSprop(decoder.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "decoder.to(device)\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    decoder.train()\n",
    "    xx,yy=random_training_set()\n",
    "    #print(xx,yy)\n",
    "    xx=xx.to(device)\n",
    "    yy=yy.to(device)\n",
    "    loss = train(xx,yy)       \n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
    "        decoder.eval()\n",
    "        print(evaluate('Ho', 400), '\\n')\n",
    "        #print(evaluate('Wa', 200), '\\n')\n",
    "        decoder.train()\n",
    "        \n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        loss_avg = 0\n",
    "        \n",
    "        \n",
    "    if epoch % 5000==0:\n",
    "        \n",
    "        path_='holmes_30k_001_drop_'+str(epoch)+'.pth'\n",
    "        torch.save(decoder.state_dict(), path_)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13m 43s (1000 3%) 1.2711]\n",
      "However that\n",
      " he must have all rather who was the only stand.\n",
      "\n",
      " \"An and you wripped that you rest quite some next good trug down,\n",
      " and the lady was nearly an oakled overtour who was a shoulds had been a\n",
      " very clear. And the same seemed of a minute face is a red in the one of cheming of our look of the face. I\n",
      " must comy mind which her years and made no Police, Mr. Holmes, he\n",
      " had every had seen our  \n",
      "\n",
      "[27m 36s (2000 6%) 1.2815]\n",
      "Hould both to tell your little an old\n",
      " Mear book, hind have you all that our work. The first man have a\n",
      " small criminal stand of the most close, and even we found away\n",
      " the might important of event, sir, which is cuning the boots in the enter\n",
      " of it in the instrection's broeger of his story. As evident which\n",
      " had been at the wrack. There was for about this very pearbing more\n",
      " things of the visitor's \n",
      "\n",
      "[41m 47s (3000 10%) 1.3176]\n",
      "How she redary, the\n",
      " removed to me. Your hundred of ubone\n",
      " discollected a tugn of cheek with the house's fact, an instant you a\n",
      " phopary. Screatic room, and the Nor is not a viful, the subject was complically\n",
      " drew to me. Her starting half--with behind down a paper and some hands\n",
      " he was a face after. As I and the last broken and some\n",
      " reply of great keep of the documents of the notigent of the\n",
      " Tre \n",
      "\n",
      "[55m 54s (4000 13%) 1.3245]\n",
      "How, for the feeling shifficial. It was\n",
      " her.\"\n",
      "\n",
      " It was once along your glance day of early have antter the hurrable purpose in his\n",
      " head from me to her the he were which had to be chatting me the\n",
      " chairer and not quite seen the develops in the dark. Therefore, what was only report.\"\n",
      "\n",
      " \"The imperson?\"\n",
      "\n",
      " \"I return me.\"\n",
      "\n",
      " \"There was a country first prodection piped to surprise the window.'\n",
      "\n",
      " \"'Of yell \n",
      "\n",
      "[69m 46s (5000 16%) 1.4848]\n",
      "Hort before he possible.\n",
      "\n",
      " \"You is every pocket. We told you at her.\"\n",
      "\n",
      " \"But I can not be said that found the companys of a concentration as the world's\n",
      " put of the other speck to do in the remarkable quarter, into a\n",
      " side, and I have insult a braws referent.\"\n",
      "\n",
      " Sherlock Holmes, and the coaple at the grassic in the other\n",
      " and a cororse of the end, and from his half-other of secret-affairs\n",
      " to cut to \n",
      "\n",
      "[83m 37s (6000 20%) 1.1061]\n",
      "Hould assure you my fition which\n",
      " told him. A perfock of the step and about the cart of the box,\n",
      " even, and treased at his house have been allous who is we should not\n",
      " come of a door. \"How he used me that you have done if he was all\n",
      " drawn,\" he such a short. \"I shall professioned to the door.\"\n",
      "\n",
      " \"At that I saw the further blindhouse?\"\n",
      "\n",
      " \"He have taken it in the trive, and a country pocket, and the\n",
      "  \n",
      "\n",
      "[97m 55s (7000 23%) 1.5857]\n",
      "Hophing of the problem of\n",
      " very said. So I have so goded his home as the inspector with his\n",
      " face, and he is such all to were pointing in brought, but the\n",
      " Alment Brows and so think had much only he contained the bany company or step.\n",
      " He accupliced it all to sure that he had a good unclusch of evening to the houses\n",
      " which had been other burning as find at the country early\n",
      " hotel through the sumpre \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 30000\n",
    "print_every = 1000\n",
    "plot_every = 1000\n",
    "hidden_size = 200\n",
    "n_layers = 3\n",
    "lr = 0.0005\n",
    "\n",
    "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "decoder.load_state_dict(torch.load('holmes_30k_001_drop_30000.pth'))\n",
    "#decoder_optimizer=torch.optim.RMSprop(decoder.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "decoder.to(device)\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    decoder.train()\n",
    "    xx,yy=random_training_set()\n",
    "    #print(xx,yy)\n",
    "    xx=xx.to(device)\n",
    "    yy=yy.to(device)\n",
    "    loss = train(xx,yy)       \n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
    "        decoder.eval()\n",
    "        print(evaluate('Ho', 400), '\\n')\n",
    "        #print(evaluate('Wa', 200), '\\n')\n",
    "        decoder.train()\n",
    "        \n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        loss_avg = 0\n",
    "        \n",
    "        \n",
    "    if epoch % 5000==0:\n",
    "        \n",
    "        path_='holmes_30k_001_drop_'+str(epoch+30000)+'.pth'\n",
    "        torch.save(decoder.state_dict(), path_)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Training Losses\n",
    "\n",
    "Plotting the historical loss from all_losses shows the network learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7b4c08b2e8>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAHhtJREFUeJzt3Xl0XGeZ5/Hvo33frMWLvAU7thMTO7GyQHAwBMgGhEDIJB0InQMYmjQdDjANzTCHnmaYaTLD0sCQtHvIhHAyCQEnJNBDutPZHOgQW47txLbkxIk3WbIlWZa12drqmT+qvGspSyWV7q3f55w6Vap6XfXcU8c/Xb33fe41d0dERMIlLdkFiIhI4incRURCSOEuIhJCCncRkRBSuIuIhJDCXUQkhBTuIiIhNGq4m9lsM3vOzOrMbJuZ3T3EmMVm9pKZ9ZrZVyemVBERiVdGHGMGgK+4+ytmVghsNLOn3X37KWPagL8CPjIRRYqIyLkZNdzdvQloij3uNLM6YBaw/ZQxzUCzmd0Q7weXl5f7vHnzzrlgEZFUtnHjxlZ3rxhtXDx77ieY2TzgYuDlsRRlZquB1QBz5syhtrZ2LG8jIpKyzGxPPOPiPqBqZgXAWuBL7t4xlqLcfY2717h7TUXFqL94RERkjOIKdzPLJBrsD7n7YxNbkoiIjFc8q2UM+BlQ5+7fn/iSRERkvOKZc78S+CTwmpltjj33DWAOgLvfZ2bTgVqgCIiY2ZeAC8Y6fSMiIuMTz2qZPwA2ypgDQHWiihIRkfFRh6qISAgp3EVEQihw4V5/oIN7nqqnvacv2aWIiExZgQv3PYd6+Onzb9Jw+GiySxERmbICF+5VRTkAHOw4luRKRESmrsCFe2VhNgAHO3qTXImIyNQVuHCviIV7c6f23EVEhhO4cM9MT6O8IEt77iIiIwhcuANUFObQoj13EZFhBTLcq4qytecuIjKCQIZ7ZWG2VsuIiIwgkOFeVZRDa1cvgxFPdikiIlNSIMO9siiHiMOhLk3NiIgMJZjhfmI5pMJdRGQogQx3damKiIwskOGuLlURkZEFMtzVpSoiMrJAhru6VEVERhbIcAd1qYqIjCSw4a4uVRGR4QU33AtztFpGRGQYo4a7mc02s+fMrM7MtpnZ3UOMMTP7kZntNLNXzeySiSn3pMqibHWpiogMI5499wHgK+6+BLgCuMvMLjhjzHXAwthtNXBvQqscgrpURUSGN2q4u3uTu78Se9wJ1AGzzhh2I/CgR/0JKDGzGQmv9hTqUhURGd45zbmb2TzgYuDlM16aBew75ecGzv4FkFDqUhURGV7c4W5mBcBa4Evu3nHmy0P8k7Mmw81stZnVmlltS0vLuVV6hqoidamKiAwnrnA3s0yiwf6Quz82xJAGYPYpP1cDjWcOcvc17l7j7jUVFRVjqfeE8oJszNSlKiIylHhWyxjwM6DO3b8/zLAngTtiq2auAI64e1MC6zxLZnoa0/LVpSoiMpSMOMZcCXwSeM3MNsee+wYwB8Dd7wP+H3A9sBPoAe5MfKlnqyjMoVlz7iIiZxk13N39Dww9p37qGAfuSlRR8aoqytZqGRGRIQS2QxXUpSoiMpxAh7u6VEVEhhbwcFeXqojIUAId7lW6IpOIyJACHe6VsS5VrXUXETldoMNdXaoiIkMLdLirS1VEZGiBDnd1qYqIDC3Q4Q5QqS5VEZGzBD/c1aUqInKWwIe7ulRFRM4W+HBXl6qIyNlCEO7qUhUROVPgw11dqiIiZwt8uKtLVUTkbIEPd3WpioicLfDhfrxLVStmREROCny4H+9S1Vp3EZGTAh/uoC5VEZEzhSPc1aUqInKaUIS7ulRFRE43arib2f1m1mxmW4d5vdTMHjezV81svZktTXyZI6tSl6qIyGni2XN/ALh2hNe/AWx294uAO4B/SEBd56RCXaoiIqcZNdzdfR3QNsKQC4BnYmPrgXlmVpWY8uKjLlURkdMlYs59C/BRADO7DJgLVCfgfeOmLlURkdMlItz/Hig1s83AF4FNwMBQA81stZnVmlltS0tLAj46Sl2qIiKnyxjvG7h7B3AngJkZsCt2G2rsGmANQE1NTcKOfqpLVUTkdOPeczezEjPLiv34GWBdLPAnjbpURURON+qeu5k9DKwCys2sAfgWkAng7vcBS4AHzWwQ2A58esKqHYG6VEVETho13N39tlFefwlYmLCKxqhKXaoiIieEokMVonvumnMXEYkKTbirS1VE5KTQhLu6VEVETgpNuKtLVUTkpPCEu7pURUROCE24V6pLVUTkhNCEu7pURUROCk24q0tVROSk0IQ7qEtVROS4UIV7VVE2B3VAVUQkXOEe3XPXtIyISKjCXV2qIiJRoQr3SnWpiogAYQt3damKiAAhC/fjXapa6y4iqS5U4X68S1Vr3UUk1YUq3NWlKiISFapwj3ap6opMIiKhCneIHlRVl6qIpLrQhbu6VEVEQhju6lIVEQlhuKtLVUQkjnA3s/vNrNnMtg7zerGZ/dbMtpjZNjO7M/Flxk9dqiIi8e25PwBcO8LrdwHb3X0ZsAr4nplljb+0sVGXqohIHOHu7uuAtpGGAIVmZkBBbOxAYso7d+pSFRFJzJz7T4AlQCPwGnC3u0eGGmhmq82s1sxqW1paEvDRZ1OXqohIYsL9GmAzMBNYDvzEzIqGGujua9y9xt1rKioqEvDRZ1OXqohIYsL9TuAxj9oJ7AIWJ+B9x+Rkl6rCXURSVyLCfS9wNYCZVQGLgLcS8L5jFu1S1bSMiKSujNEGmNnDRFfBlJtZA/AtIBPA3e8Dvg08YGavAQZ8zd1bJ6ziOKhLVURS3ajh7u63jfJ6I/CBhFWUAFVFOWxr7Eh2GSIiSRO6DlWITsu0dvUyMDjkoh0RkdALZ7gf71Lt7kt2KSIiSRHOcI91qeqgqoikqlCGu7pURSTVhTrc1aUqIqkqlOFeXpClLlURSWmhDPcMdamKSIoLZbiDulRFJLWFNtzVpSoiqSzE4a5rqYpI6gptuKtLVURSWXjDXV2qIpLCQhvuJ9a6a2pGRFJQaMP95IWydVBVRFJPaMNdXaoikspCG+7qUhWRVBbacFeXqoikstCGO8DCygJe3tWGuye7FBGRSRXqcL/p4lm81dLNpn3tyS5FRGRShTrcr79oBrmZ6fx6Y0OySxERmVShDveC7AyuXTqd325p5Fj/YLLLERGZNKOGu5ndb2bNZrZ1mNf/o5ltjt22mtmgmZUlvtSxuXlFNZ3HBnh6+8FklyIiMmni2XN/ALh2uBfd/X+4+3J3Xw78DfCCu7clqL5xe8d505hZnKOpGRFJKaOGu7uvA+IN69uAh8dVUYKlpRkfvaSaF99o0Zp3EUkZCZtzN7M8onv4axP1nonysRXVRBwe37Q/2aWIiEyKRB5Q/RDwx5GmZMxstZnVmlltS0tLAj96ZPPL81kxt5S1Gxu05l1EUkIiw/1WRpmScfc17l7j7jUVFRUJ/OjR3byimjeau3i14cikfq6ISDIkJNzNrBh4N/BEIt5vItxw0QyyM9J0YFVEUkI8SyEfBl4CFplZg5l92sw+b2afP2XYTcC/unv3RBU6XkU5mVxz4XSe3NJI74DWvItIuGWMNsDdb4tjzANEl0xOaTevqObJLY08U9fM9W+fkexyREQmTKg7VM905YJyphdpzbuIhF9KhXt6mnHTJbN44fUWnQpYREItpcId4GOXVDMYcZ7Y1JjsUkREJkzKhfuCygKWzy7h11rzLiIhlnLhDtEDqzsOdrKtsSPZpYiITIiUDPcPXTSTLK15F5EQS8lwL87L5P0XVPHE5v30DUSSXY6ISMKlZLhDdGrmcE8/z9Y3J7sUEZGES9lwX7mgnIrCbNa+oqkZEQmflA33jPQ0PnrxLJ6rb6a1qzfZ5YiIJFTKhjtEz/M+EHGe2Kw17yISLikd7udXFXJRdTFrtWpGREImpcMdogdWtzd1sK1R53kXkfBI+XD/0EUzyUw31m7UJfhEJDxSPtxL87N43xKteReRcEn5cAe4/fK5HOru4zv/vD3ZpYiIJITCHXjXwnI+8675/PylPfyqdl+yyxERGTeFe8zXr1vMlQum8Z9+s5Ut+9qTXY6IyLgo3GMy0tP48W2XUFGQzed+sZGWTjU2iUhwKdxPUZafxZo7VtB+tI+7HnpFB1hFJLAU7me4cGYx3/3YRazf3cZ/1QFWEQmoUcPdzO43s2Yz2zrCmFVmttnMtpnZC4ktcfLduHwWn105nwdf2sOjOsAqIgEUz577A8C1w71oZiXAT4EPu/uFwMcTU1pyfe3a6AHWbz6+lc06wCoiATNquLv7OqBthCF/Bjzm7ntj40NxgvSM9DR+ctslVBZl8/lfbKS581iySxIRiVsi5tzPB0rN7Hkz22hmdyTgPaeE0vws1nyyRgdYRSRwEhHuGcAK4AbgGuA/m9n5Qw00s9VmVmtmtS0tLQn46Il3wcwi7rl5GRt2H+bbv9MBVhEJhkSEewPwlLt3u3srsA5YNtRAd1/j7jXuXlNRUZGAj54cH142k9VXnccv/rSHRzfoAKuITH2JCPcngJVmlmFmecDlQF0C3ndK+etrFvGuBeV88zdbeWXv4WSXIyIyoniWQj4MvAQsMrMGM/u0mX3ezD4P4O51wFPAq8B64H+7+7DLJoMq2sF6MVXF2Xzq/vXU7h7pGLOISHKZuyflg2tqary2tjYpnz0eDYd7+OTP1tN05Cj3fWIFqxZVJrskEUkhZrbR3WtGG6cO1XNUXZrHo597B/PLC/jsg7X886tNyS5JROQsCvcxqCjM5pHVV7CsuoQvPvwKj6zfm+ySREROo3Afo+LcTH7x6ctZubCCrz/2Gv/4wpvJLklE5ASF+zjkZqXzT3fUcMNFM/jvv6/nnqfqSdYxDBGRU2Uku4Cgy8pI40e3XkxRTgY/ff5Njhzt59s3LiUtzZJdmoikMIV7AqSnGf/tprdTlJvJP77wFp3HBvjeLcvITNcfRiKSHAr3BDEz/ua6JRTnZnLPUzvo6h3gf/3ZJeRmpSe7NBFJQdq1TLAvrFrAd25aynM7mvnU/etpONyT7JJEJAUp3CfA7ZfP5Ue3Xsyr+9t53/df4MfPvMGx/sFklyUiKUThPkE+tGwmz3xlFe9dXMn3nn6dD/xgHU9vP6jVNCIyKRTuE2hWSS4/vX0FD33mcrIy0vjsg7Xc+cAGdrV2J7s0EQk5hfskuHJBOb+/eyXfvGEJtbsPc80P1vHdp+rp7h1IdmkiElIK90mSmZ7GZ1aex7NffTcfXDaDe59/k6u/9wJPbmnUVI2IJJzCfZJVFubw/VuWs/Yv3kF5YRZ/9fAmbl3zJ7Y3diS7NBEJEYV7kqyYW8YTd72L79y0lB0HO7nhxy/y5V9u1tJJEUkInc99CjjS089PX9jJ//njbnD41Dvn8oVVCyjNz0p2aSIyxcR7PneF+xTS2H6UHzz9Or9+pYGC7Ay+sGoBd145j5xMdbmKSJTCPcB2HOjknqfqeaa+melFOXz5/efzsRXVpOtkZCIpT1diCrBF0wv52Z9fyiOrr2B6cQ5/vfZVrv3hOv5NTVAiEieF+xR2xXnTePwL7+Te2y9hIOJ85sFaPn7fSzyxeb9OZyAiI9K0TED0D0b45YZ93Pv8m+xvP0pRTgY3Lp/FLTWzWTqrCDNN2YikgoTNuZvZ/cAHgWZ3XzrE66uAJ4Bdsacec/e/G+2DFe5jE4k4L711iEdr9/HU1gP0DkRYPL2QW2pm85GLZ1GmFTYioZbIcL8K6AIeHCHcv+ruHzyXAhXu43fkaD+/3dLIr2r3saXhCJnpxvuWVHFLzWxWLiwnQxcLEQmdeMN91It1uPs6M5uXiKIksYpzM/nEFXP5xBVzqT/Qwa9qG3h8035+v/UAVUXZfHzFbG69bDbVpXnJLlVEJllcc+6xcP/dCHvua4EGoJHoXvy20d5Te+4To28gwrP1B/nlhn08/3oLAO9ZVMntl89h1aJKLacUCbiErnMfJdyLgIi7d5nZ9cA/uPvCYd5nNbAaYM6cOSv27Nkz6mfL2DUc7uGXG/bxyIZ9tHT2MrM4h1svm8N/uHQ2VUU5yS5PRMZg0sJ9iLG7gRp3bx1pnPbcJ0//YIR/236Qh17eyx92tpKeZrx/SRW3XzGHK99WTpr25kUCI2Fz7nF80HTgoLu7mV1GdO38ofG+ryROZnoa1719Bte9fQa7W7t5eP3e6GqbbQeYOy2PW2pmUzO3lAtmFlGYk5nsckUkAeJZLfMwsAooBw4C3wIyAdz9PjP7S+AvgAHgKPBld//30T5Ye+7Jdax/kH/ZdoCH/rSX9bvbTjw/vzyfpbOKWTqziKWzirlwZhEleVpeKTJV6NwyEreWzl62Nh5h2/4jbN3fwWv7j7C//eiJ16tLc1k6s5i3VxezrLqEi+eUkJ897j/6RGQMJm1aRoKvojCb9yyq5D2LKk88d7i7j22NHWxtPMLW/dHbU9sOAJCeZlw4s4iauWVcNr+UFXPLqCjMTlb5IjIE7blL3DqO9bNpbzu1u9vYsLuNTXvb6R2IANHpnJq5pVw6v4xL55Uxb1qeTokgMgE0LSMTrm8gwtbGI9TubmP9rsPU7mmjvacfgPKCbFYuLGflwnLetbCcykItvRRJBIW7TLpIxHmrtYsNuw/z0puH+OPOVg519wGweHohV51fwcqF5Vw6r0wXIBEZI4W7JF0k4mxv6uDFN1p58Y0Wancfpm8wQnZGGpfNL+OqhRWsPL+cedPyyUxPU/esSBwU7jLl9PQN8PKuNta93sKLb7Sys7nrtNfTLLomP3qzE4+zMtLISDMKczJ459vKWbWoguWzS3RiNElJCneZ8hrbj/LHna20dvXRPxihfzBC32CEgUE/+fNA9PFAJMLBjl4272tnMOIU52aycmE571lUybsXVVBeoNU6khq0FFKmvJkluXy8ZvY5/ZsjPf28uLOF53dEb797tQmAi6qLWXV+BasWV7KsukRTPJLytOcugXV8Tv+5+maef72FTXsPE3EozctkXnk+2Rlp5GSmk5ORTk5mGtmx+5zMdLIz08nOSKMwJ4Pq0lzmlOVTXZqrA70y5WnPXUIvLc2ip0qYVcwXr17I4e4+XtzZygs7WmjuPEZvf4S27j6O9Q/SOxDhWP8gx/ojJ34+kxlML8phTlkec6flMXda/onHc8rydBoGCRSFu4RGaX4WH142kw8vmznqWHendyBCx7F+9rUdZW9bN3sO9bD3UA9723p4bkcLLZ0Np/2b+eX5vHdxJVcvqeTSeWVk6oCuTGGalhEZRk/fAHvbethzqIfdrd38+5uHeOnNQ/QNRijKyeDdiyq5enElqxZVaK9eJo1Wy4hMgO7eAV58o5Vn6g7y3I5mWrv6SE8zVswt5erFlVy9pIq3VeQH4tQLkYjrXP4BpHAXmWCRiLOloZ1n6pp5pr6ZuqYOIHoitqzYlI27E3FwHHdwIPpfLvrz9OIcLphRxAUzi7hgRhFLZhZRNAHn1D/WP8i2xiNs2tvO5n3RW2P7UWaV5nJeeQHzy/N5W0U+88sLOK8in+lFOQr+KUrhLjLJ9rcf5dm6g2zedwTHSTPDiB6oNSx6bwBGmkWDfl9bD9sbO06cpgFgTlneaYF/4awiphflxP3XgLuzq7X7RIhv2ttOXVMHA5Ho//VZJbksn13CnGl5NBw+yq7WLt5q6aanb/DEe+RmpjOvPJ/zyvOjYV+cw7T8LKYVZFOWn0V5fjZFuRlx1eTudPYO0N7dT1tPH4e7+zjc08e0gmyWzCikoiA7EH/pTBUKd5GAcHdaOnvZ1tjB9qYOtsfud7V2nxiTlxVdupmeFu3WTU8zMtOj9xlp0VM3ZKQbZsbu1m6OHI2ewC0/K52LYufgXz67hOVzSoY8iZu709zZy5stXexq7eatlm52tUZve9t6GIycnRMZaUZpflYs9LOYlp9NfnY67T39tHX3Re9jYT4wxL8/blp+FktmFLFkRiGLpxexZEYRCyoLyMpI7AHrwYizeV87z9YfpL6pk+LcTMrysygriG1DfvaJx2X5WRRkx/fLa7Ip3EUCrqt3gPqmaNDvbu1hIBJhIOIMDEbvByMevR+M3cden1WSGwvzUhZUFoy7oat/MLqktLWrl7buPg519XGou49DsZ9bu/po6+7lUHcf3b2DlOZlUpqXRWl+NDxL8rIoy8uiND+LsvxMSvKyKMnN5EDHMeqbOqlr6qD+QCc7DnbSF1uimpFmLKgsYMmMIhZPL2Rx7L6y8Nz28juO9bPu9RaerYv2QrR1R4+RLKgooKt3gEPdvRzrP3tZLEBWehpl+VkU5mSQn51BfnY6eVkZFGRnkJeVHn0u6+Tz0wqyuGRO6YRf20DhLiKBMjAYYVdrN3UHYoHf1EFdUycHOo6dGFOal8mi6cf38AtZNL2I86sKyMs6uar7rZYunq1v5pm6ZjbsbmMg4pTkZfKeRZW8d3ElV51fQXHuyeMaPX0DHOrqo607ejvUffKXVVtXH129A3T3DdLdO0B37wA9xx/3DQz5i+G88nwunVfGZfOjt+rS3IT+BaBwF5FQaO/po/5AJ/VNHew42EldUyevH+w8cYzADOaW5bGgsvDEtBLAoqpC3rskulz14jmlE3JKioHBCD39g/T0DrK//Wjs2gbRi9l0HBsAYEZxDpfFLmJz+fwyFlQWjCvsFe4iElqRiLPvcE8s9DupP9DBG81dzCzJ5X1LopeMnF2Wl9T6dhzsZMPuNl7e1caGXW00d/YC0b8+vrBqAZ+96rwxvbdOPyAioZWWZsydls/caflcc+H0ZJdzlrQ0ix0kLuKOd8zD3dlzqIf1u6NBX1U88VcmU7iLiEwwM2NeeT7zyvO55RzPhDpWo641MrP7zazZzLaOMu5SMxs0s5sTV56IiIxFPAtJHwCuHWmAmaUD3wX+JQE1iYjIOI0a7u6+DmgbZdgXgbVAcyKKEhGR8Rl3C5iZzQJuAu6LY+xqM6s1s9qWlpbxfrSIiAwjEf29PwS+5u6Dow109zXuXuPuNRUVFQn4aBERGUoiVsvUAI/EFuWXA9eb2YC7/yYB7y0iImMw7nB39/nHH5vZA8DvFOwiIsk1arib2cPAKqDczBqAbwGZAO4+6jy7iIhMvqSdfsDMWoA9Y/zn5UBrAsuZCsK2TWHbHgjfNoVteyB82zTU9sx191EPWiYt3MfDzGrjObdCkIRtm8K2PRC+bQrb9kD4tmk826PLt4uIhJDCXUQkhIIa7muSXcAECNs2hW17IHzbFLbtgfBt05i3J5Bz7iIiMrKg7rmLiMgIAhfuZnatme0ws51m9vVk15MIZrbbzF4zs81mFrjLUw11WmgzKzOzp83sjdh9aTJrPFfDbNPfmtn+2Pe02cyuT2aN58LMZpvZc2ZWZ2bbzOzu2POB/J5G2J4gf0c5ZrbezLbEtum/xJ6fb2Yvx76jX5pZVlzvF6RpmdiphV8H3g80ABuA29x9e1ILGycz2w3UuHsg1+ea2VVAF/Cguy+NPXcP0Obufx/7JVzq7l9LZp3nYpht+lugy93/ZzJrGwszmwHMcPdXzKwQ2Ah8BPhzAvg9jbA9txDc78iAfHfvMrNM4A/A3cCXgcfc/REzuw/Y4u73jvZ+QdtzvwzY6e5vuXsf8AhwY5JrSnnDnBb6RuDnscc/J/ofLzDiPNV1YLh7k7u/EnvcCdQBswjo9zTC9gSWR3XFfsyM3Rx4L/Dr2PNxf0dBC/dZwL5Tfm4g4F9ojAP/amYbzWx1sotJkCp3b4Lof0SgMsn1JMpfmtmrsWmbQExhnMnM5gEXAy8Tgu/pjO2BAH9HZpZuZpuJXhvjaeBNoN3dB2JD4s68oIW7DfFccOaVhnelu18CXAfcFZsSkKnnXuBtwHKgCfhecss5d2ZWQPTCOl9y945k1zNeQ2xPoL8jdx909+VANdGZiiVDDYvnvYIW7g3AqVeXrQYak1RLwrh7Y+y+GXic6JcadAdj86LH50cDf5Uudz8Y+88XAf6JgH1PsXnctcBD7v5Y7OnAfk9DbU/Qv6Pj3L0deB64Aigxs+MneYw784IW7huAhbGjx1nArcCTSa5pXMwsP3ZACDPLBz4AjHgx8oB4EvhU7PGngCeSWEtCHA/BmJsI0PcUO1j3M6DO3b9/ykuB/J6G256Af0cVZlYSe5wLvI/osYTngJtjw+L+jgK1WgYgtrTph0A6cL+7fyfJJY2LmZ1HdG8doqdg/r9B26ZTTwsNHCR6WujfAI8Cc4C9wMfdPTAHKIfZplVE/9x3YDfwuePz1VOdmb0LeBF4DYjEnv4G0XnqwH1PI2zPbQT3O7qI6AHTdKI73o+6+9/FMuIRoAzYBHzC3XtHfb+ghbuIiIwuaNMyIiISB4W7iEgIKdxFREJI4S4iEkIKdxGREFK4i4iEkMJdRCSEFO4iIiH0/wGyOgcj7SejngAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7bc09f19b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating at different \"temperatures\"\n",
    "\n",
    "In the `evaluate` function above, every time a prediction is made the outputs are divided by the \"temperature\" argument passed. Using a higher number makes all actions more equally likely, and thus gives us \"more random\" outputs. Using a lower value (less than 1) makes high probabilities contribute more. As we turn the temperature towards zero we are choosing only the most likely outputs.\n",
    "\n",
    "We can see the effects of this by adjusting the `temperature` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Professor Moriarty off the start that it day by the\n",
      " possible of the friend for the place which you may have good at the tably revark the man his see my stains\n",
      " of the spreasion and more probaculance.\"\n",
      "\n",
      " \"You can a spa\n"
     ]
    }
   ],
   "source": [
    "print(evaluate('Professor Moriarty', 200, temperature=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. watson, speating last was\n",
      " into this revorrised upon the bragan. On the voicion who was events to had been the possipies.\"\n",
      "\n",
      " \"Yive to known. \"We amaining near fisses however and a spresen, and his out of h\n"
     ]
    }
   ],
   "source": [
    "print(evaluate('Dr. watso', 200, temperature=0.8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holmes. Therhorc little at that\n",
      " were.\" Farture. Their doorece. By about Farcreck. \"Loed Tos askadling tembournesl pro\"che custeawmain.\" he past turn\n",
      " Gromaper strop. Agaying four, \"I dot your faning, I'k\n"
     ]
    }
   ],
   "source": [
    "print(evaluate('Holm', 200, temperature=1.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holmes evere an cruring of his rooms, show has hear in his hand lank by\n",
      " some remotter of about it will do Cust. Why is a fresrublabthy which is\n",
      " alread time. Once has have in visation of the faturejuch, \n"
     ]
    }
   ],
   "source": [
    "print(evaluate('Holm', 200, temperature=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holmes a devangs, as the may very bedre craw a two deterving thit after the pullress. I\n",
      " has the poplened his putterage little and him, peting which he had nevil, for he have bother, a\n",
      " that paminatered t\n"
     ]
    }
   ],
   "source": [
    "print(evaluate('Holm', 200, temperature=0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holmes, Watson,\" said Holmes came take a bloth by\n",
      " Holmes. No the proefuted to see a straggere whole a most offlition the\n",
      " side before no requarticed the case?\"\n",
      "\n",
      " \"It as have know, and he was again for a \n"
     ]
    }
   ],
   "source": [
    "print(evaluate('Holm', 200, temperature=0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holmes.\n",
      "\n",
      " \"Look anything at the paper. My adfering as new about the for to be any marking in the mittle that\n",
      " him. The cream. I am about your explessers. That have a life I langer and as my\n",
      " away. It real\n"
     ]
    }
   ],
   "source": [
    "print(evaluate('Holm', 200, temperature=0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holmes. The pliver of the so said his\n",
      " hands of fached the possain which I had been a gener. They start his proses of my sir, and\n",
      " you conture to professight that I had been the own to peess the more from\n"
     ]
    }
   ],
   "source": [
    "print(evaluate('Holm', 200, temperature=0.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holmes, and the propage of the hand in the\n",
      " man him of the not paper. The should be the pack of the more of the start of the started of the\n",
      " most as profied of the strange at the probarms start and all th\n"
     ]
    }
   ],
   "source": [
    "print(evaluate('Holm', 200, temperature=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower temperatures are less varied, choosing only the more probable outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holmes as the ready and the start of the started that he was a bention which has a man the prome and the strange and the started\n",
      " the possible of the man the possible of the stain the part of the stranger\n"
     ]
    }
   ],
   "source": [
    "print(evaluate('Holm', 200, temperature=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher temperatures more varied, choosing less probable outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holmes and the paper of the starting that\n",
      " the passion of the start that the start that the possible that the proble that the probless of\n",
      " the probless of the start and the possible of the starting that h\n"
     ]
    }
   ],
   "source": [
    "print(evaluate('Holm', 200, temperature=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercises\n",
    "\n",
    "* Train with your own dataset, e.g.\n",
    "    * Text from another author\n",
    "    * Blog posts\n",
    "    * Code\n",
    "* Increase number of layers and network size to get better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next**: [Generating Names with a Conditional Character-Level RNN](https://github.com/spro/practical-pytorch/blob/master/conditional-char-rnn/conditional-char-rnn.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
